[nltk_data] Downloading package wordnet to /usr/share/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...
tokenizer_config.json: 
 4.13k/? [00:00<00:00, 403kB/s]
tokenizer.json: 
 2.06M/? [00:00<00:00, 36.4MB/s]
special_tokens_map.json: 100%
 906/906 [00:00<00:00, 110kB/s]
config.json: 100%
 928/928 [00:00<00:00, 134kB/s]
2025-07-10 18:48:56.153424: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1752173336.528226      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1752173336.627606      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
model.safetensors.index.json: 
 25.6k/? [00:00<00:00, 2.53MB/s]
Fetching 3 files: 100%
 3/3 [01:30<00:00, 90.98s/it]
model-00001-of-00003.safetensors: 100%
 4.76G/4.76G [01:30<00:00, 49.4MB/s]
model-00002-of-00003.safetensors: 100%
 4.83G/4.83G [00:29<00:00, 213MB/s]
model-00003-of-00003.safetensors: 100%
 3.90G/3.90G [00:22<00:00, 65.8MB/s]
Loading checkpoint shards: 100%
 3/3 [00:11<00:00,  3.89s/it]
generation_config.json: 100%
 137/137 [00:00<00:00, 15.7kB/s]
README.md: 
 7.81k/? [00:00<00:00, 916kB/s]
train-00000-of-00001.parquet: 100%
 21.0M/21.0M [00:00<00:00, 81.1MB/s]
test-00000-of-00001.parquet: 100%
 20.5M/20.5M [00:00<00:00, 135MB/s]
unsupervised-00000-of-00001.parquet: 100%
 42.0M/42.0M [00:00<00:00, 204MB/s]
Generating train split: 100%
 25000/25000 [00:00<00:00, 99754.86 examples/s]
Generating test split: 100%
 25000/25000 [00:00<00:00, 151080.44 examples/s]
Generating unsupervised split: 100%
 50000/50000 [00:00<00:00, 179879.75 examples/s]
Filter: 100%
 25000/25000 [00:00<00:00, 259700.86 examples/s]
Filter: 100%
 25000/25000 [00:00<00:00, 287094.68 examples/s]
We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)
Building mask on train…
Mask indices per layer: [[0, 39, 10, 1, 34, 3, 5, 12, 22, 4], [21, 24, 30, 20, 0, 23, 5, 28, 25, 2], [5, 17, 16, 35, 7, 27, 30, 33, 12, 24], [7, 1, 15, 21, 11, 39, 10, 19, 33, 22], [10, 32, 13, 38, 34, 5, 2, 29, 9, 23], [1, 4, 7, 31, 13, 38, 17, 0, 16, 11], [30, 6, 2, 37, 29, 25, 28, 34, 7, 24], [10, 32, 23, 12, 36, 4, 13, 5, 8, 38], [23, 16, 11, 31, 3, 5, 10, 35, 29, 14], [19, 31, 8, 29, 26, 14, 25, 4, 33, 20], [19, 31, 6, 30, 14, 27, 7, 28, 16, 13], [19, 4, 22, 12, 0, 32, 8, 29, 21, 7], [5, 35, 17, 18, 4, 23, 13, 16, 33, 31], [25, 1, 7, 22, 5, 3, 35, 16, 0, 21], [11, 26, 0, 6, 15, 12, 39, 37, 24, 8], [15, 30, 3, 21, 12, 37, 14, 34, 16, 7], [14, 26, 23, 24, 3, 29, 34, 30, 11, 39], [6, 23, 1, 10, 3, 26, 37, 34, 24, 17], [28, 21, 29, 39, 19, 9, 16, 12, 24, 15], [36, 7, 5, 13, 28, 11, 29, 33, 39, 34], [9, 6, 0, 10, 15, 34, 39, 7, 35, 14], [29, 0, 31, 26, 9, 36, 7, 22, 30, 19], [4, 24, 26, 11, 21, 29, 30, 38, 33, 25], [9, 32, 4, 22, 34, 20, 21, 5, 30, 10], [39, 38, 37, 26, 15, 32, 35, 1, 30, 13], [13, 8, 21, 3, 31, 32, 14, 7, 26, 9], [29, 12, 18, 16, 20, 25, 26, 1, 33, 28], [29, 32, 25, 21, 24, 28, 39, 5, 4, 11], [23, 27, 3, 5, 35, 24, 13, 11, 14, 31], [27, 7, 3, 35, 28, 15, 29, 39, 10, 26], [20, 34, 32, 12, 11, 38, 6, 31, 25, 36], [5, 33, 9, 38, 11, 16, 19, 10, 24, 1]]

--- Baseline Gating Entropy ---

=== Gating Entropy per Layer ===
Layer 0 (model.layers.0.block_sparse_moe.router.layer): avg entropy = 2.9977
Layer 1 (model.layers.1.block_sparse_moe.router.layer): avg entropy = 2.6691
Layer 2 (model.layers.2.block_sparse_moe.router.layer): avg entropy = 3.3485
Layer 3 (model.layers.3.block_sparse_moe.router.layer): avg entropy = 3.4107
Layer 4 (model.layers.4.block_sparse_moe.router.layer): avg entropy = 3.3705
Layer 5 (model.layers.5.block_sparse_moe.router.layer): avg entropy = 3.4210
Layer 6 (model.layers.6.block_sparse_moe.router.layer): avg entropy = 3.4559
Layer 7 (model.layers.7.block_sparse_moe.router.layer): avg entropy = 3.4546
Layer 8 (model.layers.8.block_sparse_moe.router.layer): avg entropy = 3.4586
Layer 9 (model.layers.9.block_sparse_moe.router.layer): avg entropy = 3.4449
Layer 10 (model.layers.10.block_sparse_moe.router.layer): avg entropy = 3.4815
Layer 11 (model.layers.11.block_sparse_moe.router.layer): avg entropy = 3.4803
Layer 12 (model.layers.12.block_sparse_moe.router.layer): avg entropy = 3.4910
Layer 13 (model.layers.13.block_sparse_moe.router.layer): avg entropy = 3.5057
Layer 14 (model.layers.14.block_sparse_moe.router.layer): avg entropy = 3.5313
Layer 15 (model.layers.15.block_sparse_moe.router.layer): avg entropy = 3.5474
Layer 16 (model.layers.16.block_sparse_moe.router.layer): avg entropy = 3.4749
Layer 17 (model.layers.17.block_sparse_moe.router.layer): avg entropy = 3.4815
Layer 18 (model.layers.18.block_sparse_moe.router.layer): avg entropy = 3.4907
Layer 19 (model.layers.19.block_sparse_moe.router.layer): avg entropy = 3.4945
Layer 20 (model.layers.20.block_sparse_moe.router.layer): avg entropy = 3.5122
Layer 21 (model.layers.21.block_sparse_moe.router.layer): avg entropy = 3.4991
Layer 22 (model.layers.22.block_sparse_moe.router.layer): avg entropy = 3.5167
Layer 23 (model.layers.23.block_sparse_moe.router.layer): avg entropy = 3.4504
Layer 24 (model.layers.24.block_sparse_moe.router.layer): avg entropy = 3.4714
Layer 25 (model.layers.25.block_sparse_moe.router.layer): avg entropy = 3.4621
Layer 26 (model.layers.26.block_sparse_moe.router.layer): avg entropy = 3.4701
Layer 27 (model.layers.27.block_sparse_moe.router.layer): avg entropy = 3.4557
Layer 28 (model.layers.28.block_sparse_moe.router.layer): avg entropy = 3.4512
Layer 29 (model.layers.29.block_sparse_moe.router.layer): avg entropy = 3.4552
Layer 30 (model.layers.30.block_sparse_moe.router.layer): avg entropy = 3.4291
Layer 31 (model.layers.31.block_sparse_moe.router.layer): avg entropy = 3.3881
Routing counts (baseline): 100%
 500/500 [04:46<00:00,  1.92it/s]

--- Baseline Routing Distribution ---

=== Routing Distribution & Entropy ===
Layer 0: counts=[102560, 66663, 39490, 61446, 53150, 57711, 12085, 33935, 577, 47559, 73362, 7789, 56149, 35381, 4808, 46904, 7338, 36593, 52060, 7583, 1313, 7685, 53612, 1350, 4511, 29987, 8272, 944, 1395, 3708, 5138, 2230, 21792, 6044, 65971, 3175, 4892, 28282, 48926, 88150] entropy=3.229 (norm=0.875)
Layer 1: counts=[144330, 1436, 36353, 137, 0, 66009, 0, 0, 0, 15, 17980, 0, 0, 0, 0, 91, 0, 21108, 826, 34304, 148812, 148815, 20286, 138636, 148815, 41131, 0, 0, 51573, 4, 148813, 1899, 0, 0, 1022, 0, 17672, 414, 0, 39] entropy=2.469 (norm=0.669)
Layer 2: counts=[19810, 12989, 21622, 23178, 3137, 148815, 26008, 45203, 16687, 25577, 514, 7651, 34485, 8741, 6541, 16331, 68849, 104544, 27980, 31595, 20734, 33233, 19928, 11763, 33446, 33093, 25389, 44737, 29233, 32994, 43276, 13201, 22771, 37571, 12846, 66410, 13994, 17858, 18295, 9491] entropy=3.381 (norm=0.916)
Layer 3: counts=[11284, 56058, 24925, 31462, 32033, 26715, 10442, 148791, 34514, 28067, 38416, 42559, 28034, 24198, 13205, 49430, 20357, 11702, 8459, 38124, 16725, 45268, 34925, 32798, 23734, 23500, 31010, 22740, 18208, 10162, 33643, 1554, 7075, 36207, 25989, 21959, 29935, 31467, 24268, 40578] entropy=3.484 (norm=0.944)
Layer 4: counts=[36289, 30047, 44766, 191, 27813, 45785, 16088, 34209, 553, 43736, 76609, 24253, 35848, 51158, 19729, 4136, 11179, 17055, 3125, 37783, 36897, 34252, 37874, 40517, 28736, 15635, 33426, 37566, 7636, 44284, 20325, 9919, 57565, 36128, 47388, 35351, 8437, 30150, 48503, 19579] entropy=3.502 (norm=0.949)
Layer 5: counts=[41435, 69625, 27592, 37284, 67749, 11412, 19400, 62505, 20183, 35939, 37795, 39323, 7438, 59364, 28916, 5967, 41476, 47556, 39174, 30610, 7181, 36132, 29698, 14508, 30250, 11056, 15708, 8165, 28238, 8876, 8354, 59376, 21566, 31682, 11987, 10660, 25891, 16128, 50250, 34071] entropy=3.510 (norm=0.952)
Layer 6: counts=[31726, 30174, 41196, 32721, 3133, 33062, 48593, 35862, 31458, 20142, 18024, 21709, 29153, 35036, 32588, 30205, 24934, 16416, 34046, 8164, 24611, 22006, 17169, 32553, 35858, 38973, 33935, 26219, 38405, 40674, 66387, 31608, 31449, 1258, 37140, 32696, 30592, 41314, 33609, 15722] entropy=3.598 (norm=0.975)
Layer 7: counts=[10066, 29636, 11021, 31834, 49583, 44499, 23700, 10994, 42738, 14695, 73180, 23683, 52555, 45936, 17635, 33859, 31543, 18233, 12600, 33173, 10047, 14615, 7555, 53315, 31178, 8254, 29284, 24721, 35055, 27383, 23916, 17984, 70339, 30270, 28232, 10213, 51402, 36605, 36871, 32118] entropy=3.544 (norm=0.961)
Layer 8: counts=[27826, 30905, 21103, 55087, 23045, 40094, 25227, 13341, 29884, 3808, 38294, 61280, 26322, 8456, 36236, 19795, 65023, 12092, 22088, 20939, 24104, 20663, 15189, 69753, 35522, 21874, 27265, 15678, 22831, 36584, 29432, 56717, 32690, 17645, 23374, 37658, 27720, 36168, 29615, 29193] entropy=3.574 (norm=0.969)
Layer 9: counts=[24134, 13811, 35211, 31383, 44884, 22138, 39211, 34359, 58621, 8262, 29379, 6078, 21758, 9274, 45357, 9553, 33501, 8910, 37864, 66334, 41244, 22640, 34089, 40429, 7529, 44868, 53605, 26006, 27616, 55058, 12988, 61926, 11072, 43732, 22577, 30397, 5057, 34051, 16170, 19444] entropy=3.530 (norm=0.957)
Layer 10: counts=[16594, 35482, 37734, 32388, 18521, 15142, 62691, 44235, 334, 36339, 15454, 34620, 8756, 39084, 46140, 35644, 39094, 25958, 15525, 100980, 5515, 23573, 25662, 26241, 5012, 23552, 26382, 44705, 40831, 30285, 49546, 65947, 8973, 21356, 22023, 13261, 29440, 24415, 19702, 23384] entropy=3.507 (norm=0.951)
Layer 11: counts=[53150, 14887, 36082, 30106, 70115, 1256, 13651, 39951, 46368, 37746, 10118, 28745, 56225, 21637, 4237, 19366, 27565, 28271, 17990, 82113, 12647, 44603, 62692, 11881, 21012, 27996, 16216, 4482, 27720, 45061, 8045, 39796, 47539, 39810, 6687, 30744, 20779, 32627, 20922, 29682] entropy=3.494 (norm=0.947)
Layer 12: counts=[26361, 25086, 19202, 15512, 46147, 65239, 31497, 28429, 31047, 39023, 25096, 26660, 13201, 43379, 11753, 22868, 40179, 48531, 47012, 34694, 23228, 18268, 37567, 43270, 24201, 29289, 31568, 12999, 23780, 1941, 22481, 40066, 33451, 40090, 5352, 51802, 36933, 24235, 22930, 26153] entropy=3.587 (norm=0.972)
Layer 13: counts=[36078, 54529, 25685, 47588, 30046, 47936, 17657, 53696, 34122, 7512, 29925, 10838, 19907, 32210, 18582, 26216, 36919, 12235, 32943, 27643, 32288, 35819, 50247, 27321, 25045, 58556, 15268, 26264, 32824, 22906, 30249, 17529, 16668, 21157, 32928, 45175, 23391, 35197, 23148, 16273] entropy=3.603 (norm=0.977)
Layer 14: counts=[65009, 23440, 38435, 33930, 26783, 12146, 63110, 36673, 40980, 37898, 25132, 84614, 53182, 15823, 7453, 56456, 22190, 1889, 24735, 24508, 20082, 7272, 22761, 9267, 41368, 14942, 78212, 11349, 20916, 9893, 14514, 13355, 21919, 31306, 6746, 24493, 37505, 43239, 14886, 52109] entropy=3.479 (norm=0.943)
Layer 15: counts=[6515, 32937, 16405, 71656, 26511, 27520, 26283, 37489, 12775, 17146, 24689, 27104, 48659, 18021, 46544, 115891, 42770, 15025, 21374, 34967, 11030, 51416, 2198, 6632, 13742, 35994, 15957, 22205, 22692, 20185, 76966, 24471, 19331, 36356, 44726, 17786, 7442, 47947, 8178, 34985] entropy=3.464 (norm=0.939)
Layer 16: counts=[10791, 3547, 32249, 53931, 38419, 11131, 41006, 12008, 15806, 13122, 20223, 46522, 19257, 10075, 102784, 39430, 17133, 1804, 6012, 22868, 33068, 3175, 36561, 77214, 60490, 23452, 95236, 3237, 16100, 50214, 48010, 11674, 8596, 33323, 47852, 15221, 15499, 22970, 28279, 42231] entropy=3.396 (norm=0.921)
Layer 17: counts=[30744, 64465, 18477, 59383, 2298, 20673, 104224, 10990, 23405, 31519, 63404, 3293, 2661, 32254, 30004, 18542, 25232, 39907, 3445, 4077, 1419, 30001, 2582, 88992, 43696, 21905, 51714, 23841, 17152, 39318, 22174, 23634, 34026, 33081, 43613, 27746, 15840, 50081, 2667, 28041] entropy=3.398 (norm=0.921)
Layer 18: counts=[8357, 35470, 38063, 9014, 33113, 11055, 4537, 9876, 3582, 49922, 41118, 1990, 43159, 14262, 40261, 41585, 43997, 6989, 16896, 55117, 31459, 87412, 16413, 4209, 41404, 15435, 35060, 5290, 98908, 86889, 7597, 35735, 31138, 24780, 15028, 9319, 3406, 41474, 21869, 69332] entropy=3.372 (norm=0.914)
Layer 19: counts=[4188, 5675, 16337, 22939, 37356, 67167, 41106, 67750, 17986, 8785, 19232, 61301, 43856, 67321, 2015, 38676, 1702, 22988, 8312, 19129, 34875, 4296, 7743, 38341, 3568, 2342, 4570, 11707, 65216, 56170, 28951, 18134, 13264, 50870, 44246, 19705, 107321, 31636, 26859, 46885] entropy=3.364 (norm=0.912)
Layer 20: counts=[59232, 20975, 32444, 34309, 18764, 7350, 90279, 41485, 22138, 92791, 59293, 11182, 29271, 25011, 36036, 58584, 16290, 35302, 10051, 34782, 3030, 27698, 30053, 30287, 33483, 17871, 2657, 13588, 23520, 15530, 17484, 12127, 4295, 30789, 46662, 37599, 32146, 16966, 13258, 45908] entropy=3.474 (norm=0.942)
Layer 21: counts=[83074, 12834, 11344, 22880, 23359, 32814, 32536, 43758, 16726, 46713, 6440, 7159, 7267, 38506, 38620, 29081, 16596, 21486, 16491, 40149, 33145, 24838, 42928, 19234, 12210, 37852, 54276, 7449, 3661, 87367, 41861, 73981, 21092, 37663, 19595, 6999, 44605, 30184, 23510, 20237] entropy=3.484 (norm=0.944)
Layer 22: counts=[8583, 19537, 8339, 30289, 86456, 25839, 9134, 14206, 10979, 16216, 15559, 68190, 5865, 29329, 45224, 5938, 14090, 17931, 15062, 30375, 13967, 56106, 9682, 35257, 79133, 44898, 77343, 18189, 24805, 53291, 52750, 12586, 42811, 45370, 11799, 17396, 38917, 28536, 46805, 3738] entropy=3.439 (norm=0.932)
Layer 23: counts=[30055, 29345, 22536, 11775, 60190, 43031, 7469, 13835, 8677, 95885, 38738, 12231, 17532, 27518, 36518, 21661, 14298, 11206, 23225, 22104, 56771, 44947, 58947, 7132, 12578, 23058, 27021, 31791, 24205, 7123, 42232, 16110, 79819, 6452, 57210, 25104, 32928, 21784, 30902, 36577] entropy=3.486 (norm=0.945)
Layer 24: counts=[27035, 42614, 24517, 7082, 2867, 6747, 36678, 26597, 25721, 14445, 12357, 28672, 9120, 40649, 20177, 50947, 40116, 35081, 24659, 18857, 31760, 8321, 34517, 13925, 31219, 31043, 56365, 27437, 9983, 36093, 41924, 28758, 42788, 7780, 37072, 42723, 24316, 61050, 61788, 66720] entropy=3.534 (norm=0.958)
Layer 25: counts=[21781, 22225, 17563, 55760, 24574, 23913, 41387, 45085, 71682, 42007, 6838, 11085, 25509, 78678, 44638, 15267, 3406, 13560, 20228, 7406, 9473, 58441, 22866, 37720, 36145, 10815, 43992, 12204, 30965, 33337, 32976, 50189, 45404, 17957, 11185, 14988, 34559, 27969, 38626, 28117] entropy=3.517 (norm=0.953)
Layer 26: counts=[19962, 44329, 27689, 9968, 9272, 35149, 18424, 32986, 12774, 19334, 30190, 30517, 57989, 12424, 25122, 39733, 51584, 39181, 54766, 6799, 49904, 26709, 35281, 22949, 13836, 46930, 44457, 25105, 42960, 61602, 35901, 8504, 36791, 43319, 15243, 7333, 30651, 9542, 34267, 21044] entropy=3.554 (norm=0.963)
Layer 27: counts=[38806, 35022, 11666, 8638, 44697, 44962, 27502, 9614, 22438, 31821, 7449, 43060, 26901, 24682, 26479, 14186, 13755, 13255, 22872, 36029, 25237, 60322, 10118, 19078, 56429, 67103, 15022, 10767, 50143, 74843, 31776, 41614, 72181, 36090, 8114, 18120, 23218, 5858, 13835, 46818] entropy=3.500 (norm=0.949)
Layer 28: counts=[35790, 37236, 29864, 63720, 5671, 52941, 9718, 17182, 30760, 6934, 23429, 41664, 30442, 47110, 40550, 10854, 31313, 13240, 22408, 34999, 6210, 21781, 14812, 80078, 47950, 18934, 7803, 69730, 9026, 38235, 15778, 39329, 25232, 26473, 15412, 48828, 32452, 35755, 34745, 16132] entropy=3.517 (norm=0.953)
Layer 29: counts=[29262, 13323, 18155, 74323, 17674, 35078, 27162, 77769, 33226, 14458, 38377, 11547, 34396, 27601, 29418, 47118, 32130, 13757, 15025, 36633, 5213, 22793, 14484, 15904, 18839, 29394, 38071, 88392, 49192, 41849, 32637, 18618, 25749, 19653, 6100, 49705, 12498, 17832, 16372, 40793] entropy=3.518 (norm=0.954)
Layer 30: counts=[11102, 16897, 34157, 34397, 16889, 11839, 56554, 5924, 16881, 20318, 39203, 65903, 66965, 13144, 5730, 11272, 11947, 26743, 6403, 36425, 70961, 15543, 27442, 34659, 14921, 50119, 34138, 15597, 12828, 10120, 19704, 55969, 67072, 8676, 67444, 33975, 44471, 9889, 60953, 27346] entropy=3.462 (norm=0.938)
Layer 31: counts=[3866, 44654, 38686, 27640, 14984, 96897, 19294, 35277, 14771, 62094, 51975, 61383, 15248, 23964, 9136, 21159, 58583, 22943, 33811, 52062, 39131, 15932, 31462, 14378, 51263, 6216, 37313, 13942, 9262, 16289, 9889, 6426, 6184, 74837, 40105, 9714, 22309, 10539, 61499, 5403] entropy=3.431 (norm=0.930)

--- Masked Gating Entropy ---

=== Gating Entropy per Layer ===
Layer 0 (model.layers.0.block_sparse_moe.router.layer): avg entropy = 2.8142
Layer 1 (model.layers.1.block_sparse_moe.router.layer): avg entropy = 3.0199
Layer 2 (model.layers.2.block_sparse_moe.router.layer): avg entropy = 3.2564
Layer 3 (model.layers.3.block_sparse_moe.router.layer): avg entropy = 3.2138
Layer 4 (model.layers.4.block_sparse_moe.router.layer): avg entropy = 3.1929
Layer 5 (model.layers.5.block_sparse_moe.router.layer): avg entropy = 3.1901
Layer 6 (model.layers.6.block_sparse_moe.router.layer): avg entropy = 3.1847
Layer 7 (model.layers.7.block_sparse_moe.router.layer): avg entropy = 3.1627
Layer 8 (model.layers.8.block_sparse_moe.router.layer): avg entropy = 3.1814
Layer 9 (model.layers.9.block_sparse_moe.router.layer): avg entropy = 3.1499
Layer 10 (model.layers.10.block_sparse_moe.router.layer): avg entropy = 3.2024
Layer 11 (model.layers.11.block_sparse_moe.router.layer): avg entropy = 3.1924
Layer 12 (model.layers.12.block_sparse_moe.router.layer): avg entropy = 3.1761
Layer 13 (model.layers.13.block_sparse_moe.router.layer): avg entropy = 3.1866
Layer 14 (model.layers.14.block_sparse_moe.router.layer): avg entropy = 3.2310
Layer 15 (model.layers.15.block_sparse_moe.router.layer): avg entropy = 3.2300
Layer 16 (model.layers.16.block_sparse_moe.router.layer): avg entropy = 3.1426
Layer 17 (model.layers.17.block_sparse_moe.router.layer): avg entropy = 3.1740
Layer 18 (model.layers.18.block_sparse_moe.router.layer): avg entropy = 3.1669
Layer 19 (model.layers.19.block_sparse_moe.router.layer): avg entropy = 3.1878
Layer 20 (model.layers.20.block_sparse_moe.router.layer): avg entropy = 3.2327
Layer 21 (model.layers.21.block_sparse_moe.router.layer): avg entropy = 3.2273
Layer 22 (model.layers.22.block_sparse_moe.router.layer): avg entropy = 3.2384
Layer 23 (model.layers.23.block_sparse_moe.router.layer): avg entropy = 3.1762
Layer 24 (model.layers.24.block_sparse_moe.router.layer): avg entropy = 3.2034
Layer 25 (model.layers.25.block_sparse_moe.router.layer): avg entropy = 3.2139
Layer 26 (model.layers.26.block_sparse_moe.router.layer): avg entropy = 3.2317
Layer 27 (model.layers.27.block_sparse_moe.router.layer): avg entropy = 3.2226
Layer 28 (model.layers.28.block_sparse_moe.router.layer): avg entropy = 3.2121
Layer 29 (model.layers.29.block_sparse_moe.router.layer): avg entropy = 3.2457
Layer 30 (model.layers.30.block_sparse_moe.router.layer): avg entropy = 3.2232
Layer 31 (model.layers.31.block_sparse_moe.router.layer): avg entropy = 3.2365
Routing counts (masked): 100%
 500/500 [04:09<00:00,  2.25it/s]

--- Masked Routing Distribution ---

=== Routing Distribution & Entropy ===
Layer 0: counts=[0, 0, 76942, 0, 0, 0, 44225, 85981, 4608, 77679, 0, 42540, 0, 48785, 37350, 60897, 40585, 49544, 84475, 29154, 5881, 27000, 0, 6285, 28775, 51782, 30968, 4885, 7801, 8092, 28628, 15210, 77340, 14431, 0, 28428, 16548, 78500, 77201, 0] entropy=3.158 (norm=0.856)
Layer 1: counts=[0, 124315, 0, 41890, 990, 0, 11140, 55, 1943, 31368, 133245, 408, 23, 5, 3309, 35715, 1240, 140877, 61068, 120588, 0, 0, 148007, 0, 0, 0, 10258, 4, 0, 62, 0, 26333, 8949, 4593, 25466, 4576, 138407, 57659, 1116, 56911] entropy=2.619 (norm=0.710)
Layer 2: counts=[45375, 24289, 40036, 44004, 9311, 0, 39736, 0, 55621, 39989, 17802, 25878, 0, 26081, 37087, 25140, 0, 0, 46175, 72155, 33697, 52532, 36740, 26404, 0, 52440, 47683, 0, 50915, 62328, 0, 53131, 41667, 0, 32028, 0, 30954, 59098, 44454, 17770] entropy=3.333 (norm=0.903)
Layer 3: counts=[17504, 0, 26105, 43639, 42953, 32165, 28845, 0, 90312, 62209, 0, 0, 48052, 30022, 19627, 0, 35779, 19821, 15976, 0, 71975, 0, 0, 74242, 50465, 50268, 72021, 59476, 26578, 24582, 40027, 6904, 8655, 0, 56014, 43599, 45087, 25232, 22386, 0] entropy=3.266 (norm=0.885)
Layer 4: counts=[50240, 57017, 0, 4238, 41415, 0, 25405, 46138, 4685, 0, 0, 29000, 49675, 0, 33208, 14100, 29283, 34114, 25655, 60029, 42132, 48550, 56518, 0, 100407, 18390, 72619, 50520, 20049, 0, 41332, 25111, 0, 42220, 0, 44204, 10088, 69133, 0, 45045] entropy=3.256 (norm=0.883)
Layer 5: counts=[0, 0, 117867, 51419, 0, 58020, 42001, 0, 49471, 60021, 71327, 0, 16379, 0, 27711, 4150, 0, 0, 49076, 51711, 4453, 47131, 16389, 20637, 35379, 20609, 9544, 17192, 28694, 21838, 8619, 0, 55444, 56218, 33222, 13959, 36158, 63033, 0, 102848] entropy=3.179 (norm=0.862)
Layer 6: counts=[55468, 42347, 0, 34570, 6393, 36603, 0, 0, 39391, 15096, 111041, 33185, 58052, 42232, 40107, 33989, 23251, 23685, 36294, 18156, 40576, 27483, 62661, 34260, 0, 0, 82955, 24131, 0, 0, 0, 45123, 27447, 8412, 0, 26456, 31075, 0, 103523, 26558] entropy=3.242 (norm=0.879)
Layer 7: counts=[19850, 41230, 14103, 39807, 0, 0, 54994, 20614, 0, 17984, 0, 45628, 0, 0, 24900, 24608, 48308, 41945, 22888, 47707, 33312, 48090, 24686, 0, 29727, 13445, 54796, 59016, 64131, 37844, 49582, 27249, 0, 64671, 53872, 37414, 0, 62391, 0, 65728] entropy=3.316 (norm=0.899)
Layer 8: counts=[35648, 50042, 35240, 0, 32081, 0, 21835, 24078, 63157, 96134, 0, 0, 28260, 63839, 0, 21937, 0, 89543, 21860, 47473, 23602, 33117, 43932, 0, 49206, 35681, 44524, 15906, 30652, 0, 28076, 0, 43826, 30939, 30652, 0, 29015, 43337, 51155, 25773] entropy=3.306 (norm=0.896)
Layer 9: counts=[26091, 28827, 57025, 41972, 0, 23113, 86151, 70108, 0, 18217, 42129, 9399, 81176, 19278, 0, 21278, 39236, 17207, 75814, 0, 0, 105625, 33163, 41246, 13592, 0, 0, 44043, 32727, 0, 24979, 0, 25445, 0, 31918, 40382, 36091, 32779, 22137, 49372] entropy=3.251 (norm=0.881)
Layer 10: counts=[34270, 43298, 46259, 60245, 28464, 16070, 0, 0, 21956, 44694, 19351, 55713, 25633, 0, 0, 45851, 0, 57336, 47496, 0, 9680, 30323, 36143, 33017, 62677, 38268, 54513, 0, 0, 25539, 0, 0, 22895, 60851, 44343, 17125, 86493, 36329, 30124, 55564] entropy=3.309 (norm=0.897)
Layer 11: counts=[0, 20131, 62400, 39817, 0, 35501, 23635, 0, 0, 101467, 17870, 20190, 0, 41631, 8491, 24780, 47220, 37505, 78092, 0, 28571, 0, 0, 32151, 33375, 44333, 36189, 8167, 29020, 0, 18537, 51914, 0, 64877, 31760, 54766, 63202, 58708, 20283, 55937] entropy=3.267 (norm=0.886)
Layer 12: counts=[29821, 27400, 20030, 34170, 0, 0, 48661, 33830, 36794, 47950, 35563, 23364, 81936, 0, 22277, 34883, 0, 0, 0, 89535, 33176, 24400, 39809, 0, 40848, 84955, 35483, 29371, 22274, 9101, 30171, 0, 62174, 0, 33213, 0, 60879, 44282, 34525, 39645] entropy=3.300 (norm=0.894)
Layer 13: counts=[0, 0, 43468, 0, 34499, 0, 15998, 0, 99776, 8833, 32174, 34339, 27448, 38389, 26314, 43763, 0, 18732, 37484, 33473, 24780, 0, 0, 76232, 34327, 0, 16746, 56359, 49676, 25169, 38065, 129273, 23399, 58133, 51240, 0, 20062, 46434, 25510, 20425] entropy=3.239 (norm=0.878)
Layer 14: counts=[0, 75503, 47692, 31810, 45728, 20295, 0, 52805, 0, 99014, 39259, 0, 0, 23039, 12548, 0, 43677, 54892, 54280, 35409, 56688, 17225, 31194, 29187, 0, 38272, 0, 19656, 72332, 21344, 26211, 12630, 29316, 41615, 10567, 36374, 74955, 0, 37003, 0] entropy=3.270 (norm=0.886)
Layer 15: counts=[11399, 47032, 33889, 0, 39302, 126744, 16942, 0, 29668, 19309, 67512, 38330, 0, 28487, 0, 0, 0, 51106, 68123, 38554, 60224, 0, 2520, 12422, 15366, 40387, 19449, 101508, 57040, 38149, 0, 70967, 24653, 41463, 0, 22512, 10788, 0, 12314, 44361] entropy=3.185 (norm=0.863)
Layer 16: counts=[21259, 9847, 60335, 0, 69494, 29742, 80547, 19769, 36397, 18399, 15784, 0, 21349, 29665, 0, 48348, 40659, 4674, 4607, 108805, 116112, 8581, 70534, 0, 0, 28673, 0, 5048, 36478, 0, 0, 19300, 17604, 119098, 0, 42652, 48677, 35504, 22579, 0] entropy=3.110 (norm=0.843)
Layer 17: counts=[29362, 0, 13004, 0, 3318, 89469, 0, 31867, 32061, 55158, 0, 8403, 8694, 27970, 45223, 76170, 34653, 0, 8561, 5488, 2619, 85258, 3334, 0, 0, 19050, 0, 64600, 30161, 127441, 84261, 10561, 33543, 124587, 0, 37005, 32226, 0, 5530, 60943] entropy=3.035 (norm=0.823)
Layer 18: counts=[16037, 147500, 111069, 18324, 45801, 22471, 10251, 13875, 7927, 0, 62552, 1786, 0, 22508, 94779, 0, 0, 12598, 13331, 0, 31333, 0, 88718, 36592, 0, 53580, 28705, 10541, 0, 0, 13577, 76332, 27676, 46248, 11003, 74692, 4184, 55352, 31178, 0] entropy=3.051 (norm=0.827)
Layer 19: counts=[10976, 14832, 37616, 31817, 24905, 0, 113955, 0, 30645, 12824, 29520, 0, 49249, 0, 2665, 117398, 7868, 38853, 23929, 22514, 135677, 12742, 12303, 36691, 14958, 8353, 39302, 15538, 0, 0, 107041, 69535, 24781, 0, 0, 40170, 0, 67738, 36125, 0] entropy=3.072 (norm=0.833)
Layer 20: counts=[0, 43008, 84928, 29455, 55759, 30465, 0, 0, 45559, 0, 0, 33446, 81664, 52294, 0, 0, 44623, 60395, 35789, 37878, 18197, 27668, 28390, 25763, 53365, 26221, 10867, 97810, 31702, 20701, 24710, 61564, 16400, 27229, 0, 0, 23256, 41238, 20176, 0] entropy=3.276 (norm=0.888)
Layer 21: counts=[0, 13482, 34839, 47446, 43196, 31354, 40475, 0, 31805, 0, 11939, 33658, 22880, 37054, 65783, 31608, 27556, 22662, 40066, 0, 115678, 47688, 0, 38284, 100380, 42401, 0, 15112, 7734, 0, 0, 0, 27399, 48906, 31022, 23859, 0, 89194, 31521, 35539] entropy=3.242 (norm=0.879)
Layer 22: counts=[28027, 32320, 32941, 61482, 0, 44207, 15120, 13077, 42367, 54019, 30483, 0, 10289, 21044, 100008, 20071, 26531, 25342, 23547, 37862, 23089, 0, 18890, 35640, 0, 0, 0, 48960, 57863, 0, 0, 53155, 68965, 0, 30138, 37788, 51774, 40425, 0, 105096] entropy=3.259 (norm=0.884)
Layer 23: counts=[33684, 59159, 33623, 41374, 0, 0, 19351, 18045, 20118, 0, 0, 19866, 40307, 27664, 70700, 36766, 30495, 24784, 25357, 36058, 0, 0, 0, 27969, 30923, 20993, 47844, 48895, 42470, 20104, 0, 32419, 0, 30088, 0, 46825, 71522, 80592, 76015, 76510] entropy=3.299 (norm=0.894)
Layer 24: counts=[43190, 0, 25865, 18362, 14427, 23046, 114714, 42692, 21017, 35196, 24114, 46159, 19877, 0, 38861, 0, 46214, 48402, 56041, 29274, 63368, 43471, 44328, 29706, 34882, 54896, 0, 34597, 17129, 40854, 0, 53673, 0, 22595, 62065, 0, 41505, 0, 0, 0] entropy=3.297 (norm=0.894)
Layer 25: counts=[36236, 32946, 32406, 0, 39440, 22153, 48143, 0, 0, 0, 10557, 27340, 36362, 0, 0, 37255, 21116, 28308, 31485, 20736, 24656, 0, 56695, 49710, 112792, 19139, 0, 28604, 92092, 52633, 29041, 0, 0, 61565, 28134, 15525, 45051, 56803, 52070, 41527] entropy=3.276 (norm=0.888)
Layer 26: counts=[41670, 0, 57497, 18772, 18209, 39275, 26716, 35887, 56510, 23654, 53675, 33142, 0, 29244, 62556, 46819, 0, 49622, 0, 19708, 0, 81415, 44265, 41537, 19708, 0, 0, 25646, 0, 0, 65884, 23171, 44230, 0, 54125, 27787, 46012, 19995, 64887, 18902] entropy=3.311 (norm=0.898)
Layer 27: counts=[53676, 46939, 14428, 8794, 0, 0, 61582, 19063, 16260, 56661, 17052, 0, 82292, 61917, 33750, 22501, 20624, 21680, 37412, 54453, 57634, 0, 76733, 54156, 0, 0, 21308, 17157, 0, 0, 48751, 45677, 0, 73461, 8967, 82741, 34155, 15592, 25104, 0] entropy=3.235 (norm=0.877)
Layer 28: counts=[73108, 93278, 41687, 0, 8593, 0, 9660, 62501, 26480, 20507, 99058, 0, 34891, 0, 0, 23048, 30229, 23140, 67540, 49935, 5554, 23395, 20776, 0, 0, 20789, 12743, 0, 13931, 84929, 48307, 0, 40828, 34187, 16829, 0, 46958, 56334, 71084, 30221] entropy=3.198 (norm=0.867)
Layer 29: counts=[63898, 89418, 45943, 0, 26622, 46794, 96665, 0, 36514, 27401, 0, 31789, 40304, 43898, 54295, 0, 53306, 15299, 33832, 94474, 4685, 29889, 34309, 29436, 52994, 40185, 0, 0, 0, 0, 30834, 21977, 53252, 26835, 11404, 0, 12291, 16136, 25841, 0] entropy=3.245 (norm=0.880)
Layer 30: counts=[32017, 18293, 102408, 104854, 63581, 28476, 0, 18891, 29614, 38453, 107264, 0, 0, 22493, 10852, 18625, 20107, 44757, 17590, 54010, 0, 20010, 72263, 53379, 14432, 0, 35728, 16656, 15527, 8443, 36573, 0, 0, 22305, 0, 117986, 0, 14738, 0, 30195] entropy=3.141 (norm=0.852)
Layer 31: counts=[9299, 0, 142693, 45421, 20544, 0, 71341, 132499, 30921, 0, 0, 0, 37454, 42582, 17043, 37237, 0, 38016, 29064, 0, 40094, 33474, 55512, 20925, 0, 11179, 28851, 19569, 14451, 19906, 19057, 34352, 10618, 0, 63584, 30536, 56632, 69701, 0, 7965] entropy=3.150 (norm=0.854)
Filter: 100%
 25000/25000 [00:00<00:00, 287881.31 examples/s]
Filter: 100%
 25000/25000 [00:00<00:00, 297068.06 examples/s]

Masked eval:
Eval: 100%
 500/500 [08:15<00:00,  1.19it/s]

Accuracy = 54.40%

Baseline eval:
Eval: 100%
 500/500 [09:29<00:00,  1.02it/s]

Accuracy = 81.60%